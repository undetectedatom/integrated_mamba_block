import torch
import torch.nn as nn
from mamba_ssm import Mamba
from .feature_fusion import get_fusion_module


def flatten_h(x):
    B, C, H, W = x.shape
    x = x.permute(0, 3, 1, 2).reshape(B * W, C, H).permute(0, 2, 1)
    return x

def unflatten_h(x, b, w):
    BW, H, C = x.shape
    x = x.permute(0, 2, 1).reshape(b, w, C, H).permute(0, 2, 3, 1)
    return x

def flatten_w(x):
    B, C, H, W = x.shape
    x = x.permute(0, 2, 1, 3).reshape(B * H, C, W).permute(0, 2, 1)
    return x

def unflatten_w(x, b, h):
    BH, W, C = x.shape
    x = x.permute(0, 2, 1).reshape(b, h, C, W).permute(0, 2, 1, 3)
    return x


class MambaIntegrationBlock(nn.Module):
    def __init__(self, channels, mamba_d_state=16, mamba_d_conv=4, mamba_expansion=2,
                 fusion_type='FusionCrossSpatial', bidirectional=True, channel_reduction=True):
        super().__init__()
        self.channels = channels
        self.bidirectional = bidirectional
        self.fusion_type = fusion_type
        self.channel_reduction_enabled = channel_reduction
        self.mamba_channels = channels // 2 if channel_reduction else channels

        if self.channel_reduction_enabled:
            self.channel_reduction = nn.Sequential(
                nn.Conv2d(channels, self.mamba_channels, kernel_size=1),
                nn.BatchNorm2d(self.mamba_channels),
                nn.SiLU()
            )

        self.norm_mamba = nn.LayerNorm(self.mamba_channels)
        self.mamba = Mamba(
            d_model=self.mamba_channels,
            d_state=mamba_d_state,
            d_conv=mamba_d_conv,
            expand=mamba_expansion
        )

        if self.channel_reduction_enabled:
            self.channel_expansion = nn.Sequential(
                nn.Conv2d(self.mamba_channels, channels, kernel_size=1),
                nn.BatchNorm2d(channels),
                nn.SiLU()
            )

        self.conv3x3 = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, groups=channels),
            nn.Conv2d(channels, channels, kernel_size=1),
            nn.BatchNorm2d(channels),
            nn.SiLU()
        )

        self.fusion = get_fusion_module(fusion_type, channels)

    def forward(self, x):
        b, c, h, w = x.shape
        residual = x

        x_conv = self.conv3x3(x)

        if self.channel_reduction_enabled:
            x_reduced = self.channel_reduction(x)
        else:
            x_reduced = x

        x_h_flat = flatten_h(x_reduced)
        x_h_norm = self.norm_mamba(x_h_flat)
        x_h_mamba = self.mamba_wrapper(x_h_norm)
        x_h = unflatten_h(x_h_mamba, b, w)

        x_w_flat = flatten_w(x_reduced)
        x_w_norm = self.norm_mamba(x_w_flat)
        x_w_mamba = self.mamba_wrapper(x_w_norm)
        x_w = unflatten_w(x_w_mamba, b, h)

        attention = x_h.sigmoid() * x_w.sigmoid()
        x_mamba = x_reduced * attention

        if self.channel_reduction_enabled:
            x_mamba = self.channel_expansion(x_mamba)

        if self.fusion_type == 'FusionCrossSpatial':
            output = self.fusion(residual, x_conv, x_mamba)
        else:
            output = self.fusion(x_mamba, x_conv)

        return output + residual

    def mamba_wrapper(self, x):
        x_forward = self.mamba(x)
        if self.bidirectional:
            x_backward = torch.flip(x, dims=[1])
            x_backward = self.mamba(x_backward)
            x_backward = torch.flip(x_backward, dims=[1])
            return x_forward + x_backward
        return x_forward
